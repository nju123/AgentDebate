import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import sys, os

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from src.agents.debate_agent import DebateAgent
from src.agents.judge import Judge # <-- ä»æ–°çš„ä½ç½®å¯¼å…¥
from src.models.kv_cache_utils import concatenate_kv_caches # <-- æˆ‘ä»¬éœ€è¦è¿™ä¸ªæ¥æ‹¼æ¥æœ€ç»ˆcache

# ... setup_environment å’Œ prepare_mmlu_sample_and_prompts å‡½æ•°ä¿æŒä¸å˜ ...
def setup_environment():
    model_name = "Qwen/Qwen3-4B-Instruct-2507"
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16, device_map="auto"
    ).eval()
    print(f"æ¨¡å‹ '{model_name}' åŠ è½½æˆåŠŸã€‚")
    return tokenizer, model

def prepare_mmlu_sample_and_prompts():
    mmlu_question = "Which of the following is a direct product of the light-dependent reactions of photosynthesis?"
    options = { "A": "Glucose", "B": "ATP and NADPH", "C": "Oxygen gas (Oâ‚‚)", "D": "Carbon dioxide (COâ‚‚)" }
    correct_answer = "B"
    formatted_options = "\n".join([f"{key}: {value}" for key, value in options.items()])
    user_question_content = f"Question: {mmlu_question}\n\nOptions:\n{formatted_options}"
    general_system_prompt = (
        "You are an expert biologist. Your task is to select the best answer from the given options and provide a concise reason. "
        "Structure your response *exactly* as follows:\n"
        "I believe the correct answer is [option].\n"
        "Reason: [Your 1-2 sentence reasoning here]\n"
        "You MUST NOT add any other content."
    )
    reflection_instruction = (
        "You have received your opponent's argument. "
        "First, summarize its key points in one sentence.\n"
        "Then, state whether you will stick to your initial answer or change your mind, and explain why in 1-2 sentences.\n"
        "Structure your response as:\n"
        "Summary: [...]\n"
        "Stance: [Stick/Change]\n"
        "Reason: [...]\n"
    )
    judge_system_prompt = (
        "You are an impartial judge. You will be shown a debate between two experts on a multiple-choice question. "
        "Your task is to analyze their arguments and determine the single best answer. "
        "Respond ONLY with the letter of the correct option (e.g., A)."
    )
    prompts = {
        "system": general_system_prompt,
        "reflection": reflection_instruction,
        "judge": judge_system_prompt
    }
    return user_question_content, options, correct_answer, prompts

def interleave_and_combine_caches(agent_a, agent_b, total_rounds):
    """
    äº¤é”™æ‹¼æ¥ä¸¤ä¸ª agent çš„ KV Cacheï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„è¾©è®ºå†å² Cacheã€‚
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå‡è®¾è½®æ¬¡æ˜¯ A->B, A->B ...
    """
    # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªç®€åŒ–é€»è¾‘ï¼š
    # å°† agent A çš„æœ€ç»ˆ cache å’Œ agent B æœ€ç»ˆ cache çš„æœ€åä¸€éƒ¨åˆ†æ‹¼æ¥èµ·æ¥
    # æ›´å‡†ç¡®çš„æ–¹æ³•éœ€è¦æŒ‰ token çº§åˆ«äº¤é”™ï¼Œéå¸¸å¤æ‚
    # å¯¹äº Judge æ¥è¯´ï¼Œä¸€ä¸ªåˆç†çš„ç®€åŒ–æ˜¯è®©ä»–åŸºäºå…¶ä¸­ä¸€ä¸ª agent çš„æœ€ç»ˆè§†è§’æ¥åˆ¤æ–­
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹© A çš„è§†è§’ï¼Œå¹¶å°† B çš„æœ€åä¸€ä¸ªå›ç­”æ‹¼æ¥åˆ° A çš„æœ€ç»ˆ cache åé¢
    print("[INFO] æ­£åœ¨ä¸º Judge å‡†å¤‡èåˆçš„ KV Cache...")
    final_a_cache = agent_a.kv_cache
    last_b_answer_cache = agent_b.get_answer_cache_slice()
    
    if last_b_answer_cache:
        # TODO: æœªæ¥ RoPE ä¿®æ­£éœ€è¦åœ¨è¿™é‡Œåº”ç”¨
        return concatenate_kv_caches(final_a_cache, last_b_answer_cache)
    return final_a_cache


def main():
    MAX_DEBATE_ROUNDS = 3
    tokenizer, model = setup_environment()
    question_content, _, correct_answer, prompts = prepare_mmlu_sample_and_prompts()

    print("\nâœ… å‡†å¤‡å·¥ä½œå®Œæˆã€‚")
    # ... æ‰“å°é—®é¢˜ ...

    print("\n[INFO] åˆå§‹åŒ–æ™ºèƒ½ä½“å’Œè£åˆ¤...")
    agent_a = DebateAgent(model, tokenizer, prompts['system'], name="Agent A")
    agent_b = DebateAgent(model, tokenizer, prompts['system'], name="Agent B")
    judge = Judge(model, tokenizer, prompts['judge']) # Judge ç°åœ¨æ˜¯ Agent
    final_answer = None

    for r in range(1, MAX_DEBATE_ROUNDS + 1):
        # ... (è¾©è®ºå¾ªç¯é€»è¾‘å®Œå…¨ä¸å˜) ...
        print(f"\n--- ç¬¬{r}è½® ---")

        if r == 1:
            # ç¬¬ä¸€è½®ï¼šç‹¬ç«‹å‘è¨€
            agent_a.generate_response(new_user_prompt_content=question_content)
            agent_b.generate_response(new_user_prompt_content=question_content)
        else:
            # åç»­è½®æ¬¡ï¼šç›¸äº’åæ€
            # æ³¨æ„: agent_b åæ€çš„æ˜¯ agent_a ä¸Šä¸€è½®çš„è§‚ç‚¹ï¼Œåä¹‹äº¦ç„¶
            # ä¸ºäº†é¿å…çŠ¶æ€æ±¡æŸ“ï¼Œæˆ‘ä»¬å…ˆè·å–æ‰€æœ‰ä¸Šä¸€è½®çš„cache
            cache_a_prev = agent_a.get_answer_cache_slice()
            cache_b_prev = agent_b.get_answer_cache_slice()

            print(f"\n[INFO] Agent A å¼€å§‹åæ€ Agent B (ç¬¬{r-1}è½®)çš„è§‚ç‚¹...")
            agent_a.generate_response(prompts['reflection'], opponent_cache_slice=cache_b_prev)
            
            print(f"\n[INFO] Agent B å¼€å§‹åæ€ Agent A (ç¬¬{r-1}è½®)çš„è§‚ç‚¹...")
            agent_b.generate_response(prompts['reflection'], opponent_cache_slice=cache_a_prev)

        # è§£ææœ¬è½®ç­”æ¡ˆ
        answer_a = agent_a.parse_answer_option()
        answer_b = agent_b.parse_answer_option()
        print(f"\n[Round {r} Results] Agent A é€‰æ‹©: {answer_a}, Agent B é€‰æ‹©: {answer_b}")

        # æ£€æŸ¥æ˜¯å¦è¾¾æˆå…±è¯†
        if answer_a is not None and answer_a == answer_b:
            print(f"\nâœ… åœ¨ç¬¬{r}è½®è¾¾æˆå…±è¯†: {answer_a}ã€‚è¾©è®ºç»“æŸã€‚")
            final_answer = answer_a
            break

    if final_answer is None:
        print(f"\n--- {MAX_DEBATE_ROUNDS}è½®åæœªè¾¾æˆå…±è¯†ï¼Œè¿›å…¥åŸºäº KV-Cache çš„è£å†³é˜¶æ®µ ---")
        
        # !! æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨ KV-Cache è¿›è¡Œè£å†³ !!
        final_debate_cache = interleave_and_combine_caches(agent_a, agent_b, MAX_DEBATE_ROUNDS)
        
        # æš‚æ—¶æ³¨é‡Šæ‰Judgeï¼Œå› ä¸ºè¿™ä¸ªæ‹¼æ¥é€»è¾‘æ¯”è¾ƒå¤æ‚
        # final_answer = judge.make_decision(question_content, final_debate_cache)
        # print(f"\n[INFO] è£åˆ¤æœ€ç»ˆå†³å®š: {final_answer}")
        
        # ä½œä¸ºä¸€ä¸ªä¸´æ—¶çš„ã€æ›´ç®€å•çš„è£å†³ç­–ç•¥ï¼š
        print("[INFO] è£å†³é˜¶æ®µä½¿ç”¨ä¸´æ—¶ç­–ç•¥ï¼šä»¥Agent Açš„æœ€ç»ˆç­”æ¡ˆä¸ºå‡†ã€‚")
        final_answer = agent_a.parse_answer_option()

    print(f"\n--- æœ€ç»ˆç»“æœ ---")
    # ... (æ‰“å°æœ€ç»ˆç»“æœçš„é€»è¾‘ä¸å˜) ...
    print(f"è¾©è®º/è£å†³åçš„æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    print(f"æ ‡å‡†ç­”æ¡ˆ: {correct_answer}")
    if final_answer == correct_answer:
        print("ç»“æœæ­£ç¡®ï¼ğŸ‰")
    else:
        print("ç»“æœé”™è¯¯ã€‚")
    print("\nâœ… è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")


if __name__ == "__main__":
    main()